\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Szerz\IeC {\H o}i Nyilatkozat}{1}{section*.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{megirni}{3}{section*.4}}
\pgfsyspdfmark {pgfid1}{6526379}{36676039}
\pgfsyspdfmark {pgfid4}{36717604}{36691485}
\pgfsyspdfmark {pgfid5}{38437924}{36444366}
\citation{Fernandes2015API}
\citation{inproceedings}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Works}{4}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Data Mining}{4}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Dataset}{4}{subsection.7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine Learning}{5}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Artificial Neural Networks}{5}{subsection.9}}
\citation{KINDERMANN1990277}
\citation{article}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A neural network, where the circles represent the artificial neurons as nodes\relax }}{6}{figure.caption.10}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:feedforward}{{2.1}{6}{A neural network, where the circles represent the artificial neurons as nodes\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Inversion}{6}{subsection.11}}
\newlabel{para:inversion}{{2.2.2}{6}{Inversion}{subsection.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Python}{7}{section.12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Theoretical Background}{8}{chapter.13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data Mining}{8}{section.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Preprocessing}{8}{subsection.15}}
\@writefile{toc}{\contentsline {subsubsection}{Data cleaning}{9}{section*.16}}
\@writefile{toc}{\contentsline {subsubsection}{Integration and Reduction}{9}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{Transformation and Future Scaling}{9}{section*.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Data Mining Algorithms}{10}{subsection.19}}
\citation{fine2006feedforward}
\citation{tho2010perceptron}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Feedforward Neural Networks}{11}{section.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Single-Layer Perceptron}{11}{subsection.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The appropriate weights are applied to the inputs and the resulting weighted sum passed to an activation function that produces the output.\relax }}{12}{figure.caption.22}}
\newlabel{fig:perceptron}{{3.1}{12}{The appropriate weights are applied to the inputs and the resulting weighted sum passed to an activation function that produces the output.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Multi-Layer Perceptron}{12}{subsection.23}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation}{12}{section*.24}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient descent}{13}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The gradient descent is an optimization method used by backpropagation\relax }}{13}{figure.caption.26}}
\newlabel{fig:gradient}{{3.2}{13}{The gradient descent is an optimization method used by backpropagation\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Training a MLP Model}{14}{section.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Regression}{14}{subsection.28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Activation Functions}{14}{subsection.29}}
\citation{veerarajan2007numerical}
\citation{pillo2013nonlinear}
\citation{Bottou2012}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The graphs of the most popular non-linear activation functions\relax }}{15}{figure.caption.30}}
\newlabel{fig:functions}{{3.3}{15}{The graphs of the most popular non-linear activation functions\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Optimization Methods}{15}{subsection.31}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent}{15}{section*.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The difference between the process of GD and SGD methods\relax }}{15}{figure.caption.33}}
\newlabel{fig:stochastic}{{3.4}{15}{The difference between the process of GD and SGD methods\relax }{figure.caption.33}{}}
\newlabel{eq:sgd}{{3.1}{16}{Stochastic Gradient Descent}{equation.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adam}{16}{section*.35}}
\@writefile{toc}{\contentsline {subsubsection}{Limited-memory BFGS}{16}{section*.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The optimization of SGD, Adam and L-BFGS with respect of cost and iteration\relax }}{17}{figure.caption.37}}
\newlabel{fig:optimization}{{3.5}{17}{The optimization of SGD, Adam and L-BFGS with respect of cost and iteration\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Inversion}{17}{section.38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Single-Element Inversion}{17}{subsection.39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Inversion Methods}{18}{subsection.40}}
\@writefile{toc}{\contentsline {subsubsection}{WLK Inversion}{18}{section*.41}}
\newlabel{eq:wlk}{{3.2}{18}{WLK Inversion}{equation.42}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{19}{chapter.43}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Problem Statement}{19}{section.44}}
\citation{g2015learning}
\citation{bressert2012scipy}
\citation{chen2017pandas}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Used Third-Party Libraries}{20}{subsection.45}}
\@writefile{toc}{\contentsline {subsubsection}{Scikit-Learn}{20}{section*.46}}
\citation{bengfort2018applied}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Implementation}{22}{section.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A part of the summarization of the initial dataset\relax }}{23}{figure.caption.60}}
\newlabel{fig:describe1}{{4.1}{23}{A part of the summarization of the initial dataset\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Optimizing the Dataset}{23}{subsection.61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Training the Neural Network}{24}{subsection.80}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The result of the best prediction\relax }}{25}{figure.caption.103}}
\newlabel{fig:shares}{{4.2}{25}{The result of the best prediction\relax }{figure.caption.103}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Inverting the MLP}{26}{subsection.106}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Results}{27}{section.164}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The best results of the inversion\relax }}{28}{figure.caption.174}}
\newlabel{fig:best_inversion}{{4.3}{28}{The best results of the inversion\relax }{figure.caption.174}{}}
\bibstyle{plain}
\bibdata{references}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Summary}{29}{chapter.175}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{megirni}{29}{section*.176}}
\pgfsyspdfmark {pgfid6}{6526379}{36676039}
\pgfsyspdfmark {pgfid9}{36717604}{36691485}
\pgfsyspdfmark {pgfid10}{38437924}{36444366}
\bibcite{article}{1}
\bibcite{bengfort2018applied}{2}
\bibcite{Bottou2012}{3}
\bibcite{bressert2012scipy}{4}
\bibcite{chen2017pandas}{5}
\bibcite{Fernandes2015API}{6}
\bibcite{fine2006feedforward}{7}
\bibcite{g2015learning}{8}
\bibcite{KINDERMANN1990277}{9}
\bibcite{pillo2013nonlinear}{10}
\bibcite{tho2010perceptron}{11}
\bibcite{inproceedings}{12}
\bibcite{veerarajan2007numerical}{13}
