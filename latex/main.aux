\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{zaki2010advances}
\citation{han2011data}
\citation{Fernandes2015API}
\citation{onlinenews-url}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Works}{7}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Data Mining}{7}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Dataset}{7}{subsection.5}}
\citation{uci-url}
\citation{Ren2015PredictingAE}
\citation{mitchell1997machine}
\citation{michalski2014machine}
\citation{alpaydin2009introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Online News Popularity dataset of Mashable news is served by UCI Machine Learning Repository.\relax }}{8}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mashable}{{2.1}{8}{The Online News Popularity dataset of Mashable news is served by UCI Machine Learning Repository.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine Learning}{8}{section.7}}
\citation{feldman2013neural}
\citation{priddy2005artificial}
\citation{anastassiou2011intelligent}
\@writefile{toc}{\contentsline {subsubsection}{Supervised Learning}{9}{section*.8}}
\newlabel{para:supervised}{{2.2}{9}{Supervised Learning}{section*.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Unsupervised Learning}{9}{section*.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Biological Nervous Systems}{9}{subsection.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The biological neuron's structure\relax }}{9}{figure.caption.11}}
\newlabel{fig:neuron}{{2.2}{9}{The biological neuron's structure\relax }{figure.caption.11}{}}
\citation{KINDERMANN1990277}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Artificial Neural Networks}{10}{subsection.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Neural Network Inversion}{10}{subsection.13}}
\newlabel{para:inversion}{{2.2.3}{10}{Neural Network Inversion}{subsection.13}{}}
\citation{article}
\citation{vanderplas2016python}
\citation{python-url}
\citation{van2011introduction}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Python}{11}{section.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Python and its library Scikit-Learn with the assistance of Anaconda are appropriate platforms to train artificial neural networks\relax }}{11}{figure.caption.15}}
\newlabel{fig:python_scikit}{{2.3}{11}{Python and its library Scikit-Learn with the assistance of Anaconda are appropriate platforms to train artificial neural networks\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Theoretical Background}{13}{chapter.16}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Feature Engineering}{13}{section.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Stages of data mining\relax }}{13}{figure.caption.18}}
\newlabel{fig:stages}{{3.1}{13}{Stages of data mining\relax }{figure.caption.18}{}}
\citation{pyle1999data}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Data Mining}{14}{subsection.19}}
\citation{zheng2018feature}
\citation{dong2018feature}
\@writefile{toc}{\contentsline {subsubsection}{Transformation and Feature Scaling}{15}{section*.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Data mining techniques can be used to convert data into understandable form.\relax }}{15}{figure.caption.21}}
\newlabel{fig:data_mining}{{3.2}{15}{Data mining techniques can be used to convert data into understandable form.\relax }{figure.caption.21}{}}
\newlabel{eq:min-max}{{3.1}{15}{Transformation and Feature Scaling}{equation.22}{}}
\newlabel{eq:mean}{{3.2}{15}{Transformation and Feature Scaling}{equation.23}{}}
\citation{pujari2001data}
\newlabel{eq:standard}{{3.3}{16}{Transformation and Feature Scaling}{equation.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{Algorithms of Data Mining}{16}{section*.25}}
\citation{graybill1994regression}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Regression Analysis}{17}{section.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Regression is used to find the best fit of the training data by adapting a function to the dataset\relax }}{17}{figure.caption.27}}
\newlabel{fig:regression}{{3.3}{17}{Regression is used to find the best fit of the training data by adapting a function to the dataset\relax }{figure.caption.27}{}}
\newlabel{eq:regr}{{3.4}{17}{Regression Analysis}{equation.28}{}}
\citation{allen2007understanding}
\citation{wolberg2006data}
\citation{tho2010perceptron}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Method of Least Squares}{18}{subsection.29}}
\newlabel{eq:least1}{{3.5}{18}{Method of Least Squares}{equation.30}{}}
\newlabel{eq:least2}{{3.6}{18}{Method of Least Squares}{equation.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Perceptron}{19}{section.32}}
\newlabel{eq:slp}{{3.7}{19}{Perceptron}{equation.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The appropriate weights are applied to the inputs and the resulting weighted sum passed to an activation function that produces the output.\relax }}{19}{figure.caption.34}}
\newlabel{fig:perceptron}{{3.4}{19}{The appropriate weights are applied to the inputs and the resulting weighted sum passed to an activation function that produces the output.\relax }{figure.caption.34}{}}
\newlabel{eq:slp-output}{{3.8}{19}{Perceptron}{equation.35}{}}
\citation{fine2006feedforward}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Feedforward Neural Networks}{20}{subsection.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A feedforward neural network, that does not contain any feedback connections between the nodes.\relax }}{20}{figure.caption.37}}
\newlabel{fig:feedforward}{{3.5}{20}{A feedforward neural network, that does not contain any feedback connections between the nodes.\relax }{figure.caption.37}{}}
\newlabel{eq:feedforward}{{3.9}{20}{Feedforward Neural Networks}{equation.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Multi-Layer Perceptron}{20}{subsection.39}}
\citation{chauvin2013backpropagation}
\newlabel{eq:mlp}{{3.10}{21}{Multi-Layer Perceptron}{equation.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation}{21}{section*.41}}
\citation{anderson1995introduction}
\newlabel{eq:backprop}{{3.11}{22}{Backpropagation}{equation.42}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient descent}{22}{section*.43}}
\newlabel{eq:lin_grad}{{3.12}{22}{Gradient descent}{equation.44}{}}
\newlabel{eq:cost}{{3.13}{22}{Gradient descent}{equation.45}{}}
\newlabel{eq:partial}{{3.14}{22}{Gradient descent}{equation.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The gradient descent is an optimization method used by backpropagation\relax }}{23}{figure.caption.47}}
\newlabel{fig:gradient}{{3.6}{23}{The gradient descent is an optimization method used by backpropagation\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training a MLP Model}{23}{section.48}}
\citation{pillo2013nonlinear}
\citation{veerarajan2007numerical}
\citation{sathasivam2003optimization}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Activation Functions}{24}{subsection.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The graphs of the most popular non-linear activation functions\relax }}{24}{figure.caption.50}}
\newlabel{fig:functions}{{3.7}{24}{The graphs of the most popular non-linear activation functions\relax }{figure.caption.50}{}}
\newlabel{eq:activation}{{3.15}{24}{Activation Functions}{equation.51}{}}
\newlabel{eq:activation2}{{3.16}{24}{Activation Functions}{equation.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Optimization Methods}{25}{subsection.53}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent}{25}{section*.54}}
\newlabel{eq:sgd}{{3.17}{25}{Stochastic Gradient Descent}{equation.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The difference between the process of GD and SGD methods\relax }}{25}{figure.caption.56}}
\newlabel{fig:stochastic}{{3.8}{25}{The difference between the process of GD and SGD methods\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adaptive Moment Estimation}{25}{section*.57}}
\newlabel{eq:adam1}{{3.18}{26}{Adaptive Moment Estimation}{equation.58}{}}
\newlabel{eq:adam2}{{3.19}{26}{Adaptive Moment Estimation}{equation.59}{}}
\newlabel{eq:adam3}{{3.20}{26}{Adaptive Moment Estimation}{equation.60}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limited-memory BFGS}{26}{section*.61}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The optimization of SGD, Adam and L-BFGS with respect of cost and iteration\relax }}{26}{figure.caption.62}}
\newlabel{fig:optimization}{{3.9}{26}{The optimization of SGD, Adam and L-BFGS with respect of cost and iteration\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Inversion}{27}{section.63}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Injective function means a one-to-one mapping from $X$ to $Y$.\relax }}{27}{figure.caption.64}}
\newlabel{fig:injective}{{3.10}{27}{Injective function means a one-to-one mapping from $X$ to $Y$.\relax }{figure.caption.64}{}}
\newlabel{para:surj}{{3.5}{27}{Inversion}{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces In case of surjection, more $x$ can map to the same $y$.\relax }}{27}{figure.caption.65}}
\newlabel{fig:surjective}{{3.11}{27}{In case of surjection, more $x$ can map to the same $y$.\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces If a function is bijective, every $x$ corresponds to one, and only one $y$.\relax }}{27}{figure.caption.66}}
\newlabel{fig:bijective}{{3.12}{27}{If a function is bijective, every $x$ corresponds to one, and only one $y$.\relax }{figure.caption.66}{}}
\citation{KINDERMANN1990277}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Single Element Inversion Methods}{28}{subsection.67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Williams-Linder-Kindermann Inversion}{29}{subsection.68}}
\newlabel{para:wlk-inv}{{3.5.2}{29}{Williams-Linder-Kindermann Inversion}{subsection.68}{}}
\newlabel{eq:wlk}{{3.21}{29}{Williams-Linder-Kindermann Inversion}{equation.69}{}}
\newlabel{eq:wlk_delta}{{3.22}{29}{Williams-Linder-Kindermann Inversion}{equation.70}{}}
\newlabel{eq:wlk_forward}{{3.23}{29}{Williams-Linder-Kindermann Inversion}{equation.71}{}}
\citation{uci-url}
\citation{karayiannis2013artificial}
\citation{jolly2018machine}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{30}{chapter.72}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Problem Statement}{30}{section.73}}
\citation{scipy-url}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Used Third-Party Libraries}{31}{subsection.74}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The used scientific third-party packages for Python\relax }}{31}{figure.caption.75}}
\newlabel{fig:scipy}{{4.1}{31}{The used scientific third-party packages for Python\relax }{figure.caption.75}{}}
\citation{Pedregosa2011ScikitlearnML}
\citation{scikit-url}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Implementation}{35}{section.78}}
\newlabel{fig:describe1}{{\caption@xref {fig:describe1}{ on input line 191}}{36}{The Implementation}{figure.caption.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Optimizing the Dataset}{36}{subsection.94}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Training the Neural Network}{37}{subsection.113}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The result of the multi-layer perceptron training\relax }}{39}{figure.caption.145}}
\newlabel{fig:plot}{{4.2}{39}{The result of the multi-layer perceptron training\relax }{figure.caption.145}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Inverting the MLP}{39}{subsection.146}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Results}{43}{section.269}}
\bibstyle{plain}
\bibdata{references}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Summary}{44}{chapter.280}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{onlinenews-url}{1}
\bibcite{python-url}{2}
\bibcite{scikit-url}{3}
\bibcite{scipy-url}{4}
\bibcite{uci-url}{5}
\bibcite{article}{6}
\bibcite{allen2007understanding}{7}
\bibcite{alpaydin2009introduction}{8}
\bibcite{anastassiou2011intelligent}{9}
\bibcite{anderson1995introduction}{10}
\bibcite{chauvin2013backpropagation}{11}
\bibcite{dong2018feature}{12}
\bibcite{feldman2013neural}{13}
\bibcite{Fernandes2015API}{14}
\bibcite{fine2006feedforward}{15}
\bibcite{graybill1994regression}{16}
\bibcite{han2011data}{17}
\bibcite{jolly2018machine}{18}
\bibcite{karayiannis2013artificial}{19}
\bibcite{KINDERMANN1990277}{20}
\bibcite{michalski2014machine}{21}
\bibcite{mitchell1997machine}{22}
\bibcite{Pedregosa2011ScikitlearnML}{23}
\bibcite{pillo2013nonlinear}{24}
\bibcite{priddy2005artificial}{25}
\bibcite{pujari2001data}{26}
\bibcite{pyle1999data}{27}
\bibcite{Ren2015PredictingAE}{28}
\bibcite{sathasivam2003optimization}{29}
\bibcite{tho2010perceptron}{30}
\bibcite{van2011introduction}{31}
\bibcite{vanderplas2016python}{32}
\bibcite{veerarajan2007numerical}{33}
\bibcite{wolberg2006data}{34}
\bibcite{zaki2010advances}{35}
\bibcite{zheng2018feature}{36}
\@writefile{toc}{\contentsline {chapter}{Media Instruction Manual}{48}{lstnumber.290}}
