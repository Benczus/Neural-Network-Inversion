\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Szerz\IeC {\H o}i Nyilatkozat}{1}{section*.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Fernandes2015API}
\citation{inproceedings}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Works}{4}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Data Mining}{4}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Dataset (?)}{4}{subsection.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine Learning}{5}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Artificial Neural Networks}{5}{subsection.8}}
\citation{KINDERMANN1990277}
\citation{article}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A neural network, where the circles represent the artificial neurons as nodes\relax }}{6}{figure.caption.9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:feedforward}{{2.1}{6}{A neural network, where the circles represent the artificial neurons as nodes\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Inversion}{6}{subsection.10}}
\newlabel{para:inversion}{{2.2.2}{6}{Inversion}{subsection.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Python}{7}{section.11}}
\citation{fine2006feedforward}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Theoretical Background}{8}{chapter.12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data Mining}{8}{section.13}}
\@writefile{tdo}{\contentsline {todo}{mathematical background, tools......}{8}{section*.14}}
\pgfsyspdfmark {pgfid1}{7640491}{28546895}
\@writefile{tdo}{\contentsline {todo}{data mining techniques, crucial concepts of data mining}{8}{section*.15}}
\pgfsyspdfmark {pgfid6}{7878422}{28546895}
\@writefile{tdo}{\contentsline {todo}{data cleaning (?)}{8}{section*.16}}
\pgfsyspdfmark {pgfid11}{8116353}{28546895}
\pgfsyspdfmark {pgfid4}{36717604}{28562341}
\pgfsyspdfmark {pgfid5}{38437924}{28315222}
\pgfsyspdfmark {pgfid9}{36717604}{24539547}
\pgfsyspdfmark {pgfid10}{38437924}{24292428}
\pgfsyspdfmark {pgfid14}{36717604}{16812091}
\pgfsyspdfmark {pgfid15}{38437924}{16564972}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Feedforward Neural Networks}{8}{section.17}}
\citation{tho2010perceptron}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Single-Layer Perceptron}{9}{subsection.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The appropriate weights are applied to the inputs and the resulting weighted sum passed to an activation function that produces the output.\relax }}{9}{figure.caption.19}}
\newlabel{fig:perceptron}{{3.1}{9}{The appropriate weights are applied to the inputs and the resulting weighted sum passed to an activation function that produces the output.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Multi-Layer Perceptron}{9}{subsection.20}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation}{10}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient descent}{10}{section*.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The gradient descent is an optimization method used by backpropagation\relax }}{11}{figure.caption.23}}
\newlabel{fig:gradient}{{3.2}{11}{The gradient descent is an optimization method used by backpropagation\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Training a MLP Model}{11}{section.24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Regression}{11}{subsection.25}}
\citation{veerarajan2007numerical}
\citation{pillo2013nonlinear}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Activation Functions}{12}{subsection.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The graphs of the most popular non-linear activation functions\relax }}{12}{figure.caption.27}}
\newlabel{fig:functions}{{3.3}{12}{The graphs of the most popular non-linear activation functions\relax }{figure.caption.27}{}}
\citation{Bottou2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Optimization Methods}{13}{subsection.28}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent}{13}{section*.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The difference between the process of the gradient descent and the stochastic gradient descent methods\relax }}{13}{figure.caption.30}}
\newlabel{fig:stochastic}{{3.4}{13}{The difference between the process of the gradient descent and the stochastic gradient descent methods\relax }{figure.caption.30}{}}
\newlabel{eq:sgd}{{3.1}{13}{Stochastic Gradient Descent}{equation.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adam}{13}{section*.32}}
\@writefile{toc}{\contentsline {subsubsection}{Limited-memory BFGS}{14}{section*.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The optimization of SGD, Adam and L-BFGS with respect of cost and iteration\relax }}{14}{figure.caption.34}}
\newlabel{fig:optimization}{{3.5}{14}{The optimization of SGD, Adam and L-BFGS with respect of cost and iteration\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Inversion}{14}{section.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Single-Element Inversion}{15}{subsection.36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Inversion Methods}{15}{subsection.37}}
\@writefile{toc}{\contentsline {subsubsection}{WLK Inversion}{15}{section*.38}}
\newlabel{eq:wlk}{{3.2}{16}{WLK Inversion}{equation.39}{}}
\@writefile{toc}{\contentsline {subsubsection}{Nearest Inversion}{16}{section*.40}}
\@writefile{tdo}{\contentsline {todo}{constrained optimization problem (????)}{16}{section*.41}}
\pgfsyspdfmark {pgfid16}{7640491}{16472730}
\pgfsyspdfmark {pgfid19}{36717604}{16488176}
\pgfsyspdfmark {pgfid20}{38437924}{16241057}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Problem Statement}{16}{section.42}}
\@writefile{tdo}{\contentsline {todo}{describing the problem -> inversion!!}{16}{section*.43}}
\pgfsyspdfmark {pgfid21}{6526379}{12610183}
\pgfsyspdfmark {pgfid24}{36717604}{11394726}
\pgfsyspdfmark {pgfid25}{38437924}{11147607}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{17}{chapter.44}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Python}{17}{section.45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Scikit-Learn}{17}{subsection.46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Training Libraries}{17}{subsection.47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Inversion Libraries}{17}{subsection.48}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Implementation (?)}{17}{section.49}}
\@writefile{tdo}{\contentsline {todo}{new section name (?)}{17}{section*.50}}
\pgfsyspdfmark {pgfid26}{6526379}{21095201}
\pgfsyspdfmark {pgfid29}{36717604}{21110647}
\pgfsyspdfmark {pgfid30}{38437924}{20863528}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Optimising/Analysing the Dataset (?)}{17}{subsection.51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Training the Neural Network}{17}{subsection.52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Inverting the MLP}{17}{subsection.53}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Results}{17}{section.54}}
\bibstyle{plain}
\bibdata{references}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Summary}{18}{chapter.55}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{article}{1}
\bibcite{Bottou2012}{2}
\bibcite{Fernandes2015API}{3}
\bibcite{fine2006feedforward}{4}
\bibcite{KINDERMANN1990277}{5}
\bibcite{tho2010perceptron}{6}
\bibcite{inproceedings}{7}
