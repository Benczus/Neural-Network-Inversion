\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Szerz\IeC {\H o}i Nyilatkozat}{1}{section*.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{megirni}{3}{section*.4}}
\pgfsyspdfmark {pgfid1}{6526379}{36676039}
\pgfsyspdfmark {pgfid4}{36717604}{36691485}
\pgfsyspdfmark {pgfid5}{38437924}{36444366}
\citation{zaki2010advances}
\citation{han2011data}
\citation{Fernandes2015API}
\citation{Ren2015PredictingAE}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Works}{4}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Data Mining}{4}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Dataset}{4}{subsection.7}}
\citation{mitchell1997machine}
\citation{michalski2014machine}
\citation{alpaydin2009introduction}
\citation{priddy2005artificial}
\citation{anastassiou2011intelligent}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine Learning}{5}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Artificial Neural Networks}{5}{subsection.9}}
\citation{KINDERMANN1990277}
\citation{article}
\citation{van2011introduction}
\citation{vanderplas2016python}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A neural network, where the circles represent the artificial neurons as nodes\relax }}{6}{figure.caption.10}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:feedforward}{{2.1}{6}{A neural network, where the circles represent the artificial neurons as nodes\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}ANN Inversion}{6}{subsection.11}}
\newlabel{para:inversion}{{2.2.2}{6}{ANN Inversion}{subsection.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Python}{6}{section.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Python and its library Scikit-Learn are appropriate platforms to make artificial neural networks\relax }}{7}{figure.caption.13}}
\newlabel{fig:python_scikit}{{2.2}{7}{Python and its library Scikit-Learn are appropriate platforms to make artificial neural networks\relax }{figure.caption.13}{}}
\citation{pyle1999data}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Theoretical Background}{8}{chapter.14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Feature Engineering}{8}{section.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Data Mining}{8}{subsection.16}}
\citation{zheng2018feature}
\citation{dong2018feature}
\@writefile{toc}{\contentsline {subsubsection}{Transformation and Feature Scaling}{9}{section*.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Data mining techniques can be used to convert data into understandable form.\relax }}{9}{figure.caption.18}}
\newlabel{fig:data_mining}{{3.1}{9}{Data mining techniques can be used to convert data into understandable form.\relax }{figure.caption.18}{}}
\citation{pujari2001data}
\newlabel{eq:min-max}{{3.1}{10}{Transformation and Feature Scaling}{equation.19}{}}
\newlabel{eq:mean}{{3.2}{10}{Transformation and Feature Scaling}{equation.20}{}}
\newlabel{eq:standard}{{3.3}{10}{Transformation and Feature Scaling}{equation.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data Mining Algorithms}{10}{section*.22}}
\citation{tho2010perceptron}
\citation{fine2006feedforward}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Single-Layer Perceptron}{11}{section.23}}
\newlabel{eq:slp}{{3.4}{11}{Single-Layer Perceptron}{equation.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The appropriate weights are applied to the inputs and the resulting weighted sum passed to an activation function that produces the output.\relax }}{11}{figure.caption.25}}
\newlabel{fig:perceptron}{{3.2}{11}{The appropriate weights are applied to the inputs and the resulting weighted sum passed to an activation function that produces the output.\relax }{figure.caption.25}{}}
\newlabel{eq:slp-output}{{3.5}{11}{Single-Layer Perceptron}{equation.26}{}}
\citation{chauvin2013backpropagation}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Feedforward Neural Networks}{12}{section.27}}
\newlabel{eq:feedforward}{{3.6}{12}{Feedforward Neural Networks}{equation.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Multi-Layer Perceptron}{12}{subsection.29}}
\newlabel{eq:mlp}{{3.7}{12}{Multi-Layer Perceptron}{equation.30}{}}
\citation{anderson1995introduction}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation}{13}{section*.31}}
\newlabel{eq:backprop}{{3.8}{13}{Backpropagation}{equation.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient descent}{13}{section*.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The gradient descent is an optimization method used by backpropagation\relax }}{13}{figure.caption.34}}
\newlabel{fig:gradient}{{3.3}{13}{The gradient descent is an optimization method used by backpropagation\relax }{figure.caption.34}{}}
\citation{allen2007understanding}
\citation{pillo2013nonlinear}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training a MLP Model}{14}{section.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Regression}{14}{subsection.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Regression is used to find the best fit of the training data\relax }}{14}{figure.caption.37}}
\newlabel{fig:regression}{{3.4}{14}{Regression is used to find the best fit of the training data\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Activation Functions}{14}{subsection.38}}
\citation{veerarajan2007numerical}
\citation{pillo2013nonlinear}
\citation{sathasivam2003optimization}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The graphs of the most popular non-linear activation functions\relax }}{15}{figure.caption.39}}
\newlabel{fig:functions}{{3.5}{15}{The graphs of the most popular non-linear activation functions\relax }{figure.caption.39}{}}
\newlabel{eq:activation}{{3.9}{15}{Activation Functions}{equation.40}{}}
\newlabel{eq:activation2}{{3.10}{15}{Activation Functions}{equation.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Optimization Methods}{15}{subsection.42}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent}{15}{section*.43}}
\newlabel{eq:sgd}{{3.11}{15}{Stochastic Gradient Descent}{equation.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The difference between the process of GD and SGD methods\relax }}{16}{figure.caption.45}}
\newlabel{fig:stochastic}{{3.6}{16}{The difference between the process of GD and SGD methods\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adaptive Moment Estimation}{16}{section*.46}}
\newlabel{eq:adam1}{{3.12}{16}{Adaptive Moment Estimation}{equation.47}{}}
\newlabel{eq:adam2}{{3.13}{16}{Adaptive Moment Estimation}{equation.48}{}}
\newlabel{eq:adam3}{{3.14}{16}{Adaptive Moment Estimation}{equation.49}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limited-memory BFGS}{16}{section*.50}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The optimization of SGD, Adam and L-BFGS with respect of cost and iteration\relax }}{17}{figure.caption.51}}
\newlabel{fig:optimization}{{3.7}{17}{The optimization of SGD, Adam and L-BFGS with respect of cost and iteration\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Inversion}{17}{section.52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Single-Element Inversion Methods}{17}{subsection.53}}
\citation{KINDERMANN1990277}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Williams-Linder-Kindermann Inversion}{18}{subsection.54}}
\newlabel{para:wlk-inv}{{3.5.2}{18}{Williams-Linder-Kindermann Inversion}{subsection.54}{}}
\newlabel{eq:wlk}{{3.15}{18}{Williams-Linder-Kindermann Inversion}{equation.55}{}}
\newlabel{eq:wlk_delta}{{3.16}{18}{Williams-Linder-Kindermann Inversion}{equation.56}{}}
\newlabel{eq:wlk_forward}{{3.17}{18}{Williams-Linder-Kindermann Inversion}{equation.57}{}}
\citation{karayiannis2013artificial}
\citation{nazari1992implementation}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{19}{chapter.58}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Problem Statement}{19}{section.59}}
\citation{bressert2012scipy}
\citation{chen2017pandas}
\citation{Pedregosa2011ScikitlearnML}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Used Third-Party Libraries}{20}{subsection.60}}
\@writefile{toc}{\contentsline {subsubsection}{Scikit-Learn}{20}{section*.61}}
\citation{bengfort2018applied}
\citation{jolly2018machine}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Implementation}{23}{section.62}}
\newlabel{fig:describe1}{{\caption@xref {fig:describe1}{ on input line 160}}{23}{The Implementation}{figure.caption.77}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Optimizing the Dataset}{24}{subsection.78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Training the Neural Network}{25}{subsection.97}}
\@writefile{tdo}{\contentsline {todo}{hidden layereket beirni, ha lefutottak}{25}{section*.98}}
\pgfsyspdfmark {pgfid6}{18711319}{37980456}
\pgfsyspdfmark {pgfid9}{36717604}{37995902}
\pgfsyspdfmark {pgfid10}{38437924}{37748783}
\@writefile{tdo}{\contentsline {todo}{kicserelni, ha lefutottak az algoritmusok}{25}{section*.121}}
\pgfsyspdfmark {pgfid11}{16083909}{7694960}
\pgfsyspdfmark {pgfid14}{36717604}{7710406}
\pgfsyspdfmark {pgfid15}{38437924}{7463287}
\@writefile{tdo}{\contentsline {todo}{kicserelni a kepet, ha lefutottak}{26}{section*.125}}
\pgfsyspdfmark {pgfid16}{26832769}{43629406}
\pgfsyspdfmark {pgfid19}{36717604}{43644852}
\pgfsyspdfmark {pgfid20}{38437924}{43397733}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The result of the best prediction\relax }}{26}{figure.caption.126}}
\newlabel{fig:shares}{{4.1}{26}{The result of the best prediction\relax }{figure.caption.126}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Inverting the MLP}{26}{subsection.127}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Results}{28}{section.207}}
\@writefile{tdo}{\contentsline {todo}{kicserelni majd, ha lefutottak az algoritmusok}{28}{section*.208}}
\pgfsyspdfmark {pgfid21}{11446688}{22688175}
\pgfsyspdfmark {pgfid24}{36717604}{22703621}
\pgfsyspdfmark {pgfid25}{38437924}{22456502}
\bibstyle{plain}
\bibdata{references}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Summary}{29}{chapter.219}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{megirni}{29}{section*.220}}
\pgfsyspdfmark {pgfid26}{6526379}{36676039}
\pgfsyspdfmark {pgfid29}{36717604}{36691485}
\pgfsyspdfmark {pgfid30}{38437924}{36444366}
\bibcite{article}{1}
\bibcite{allen2007understanding}{2}
\bibcite{alpaydin2009introduction}{3}
\bibcite{anastassiou2011intelligent}{4}
\bibcite{anderson1995introduction}{5}
\bibcite{bengfort2018applied}{6}
\bibcite{bressert2012scipy}{7}
\bibcite{chauvin2013backpropagation}{8}
\bibcite{chen2017pandas}{9}
\bibcite{dong2018feature}{10}
\bibcite{Fernandes2015API}{11}
\bibcite{fine2006feedforward}{12}
\bibcite{han2011data}{13}
\bibcite{jolly2018machine}{14}
\bibcite{karayiannis2013artificial}{15}
\bibcite{KINDERMANN1990277}{16}
\bibcite{michalski2014machine}{17}
\bibcite{mitchell1997machine}{18}
\bibcite{nazari1992implementation}{19}
\bibcite{Pedregosa2011ScikitlearnML}{20}
\bibcite{pillo2013nonlinear}{21}
\bibcite{priddy2005artificial}{22}
\bibcite{pujari2001data}{23}
\bibcite{pyle1999data}{24}
\bibcite{Ren2015PredictingAE}{25}
\bibcite{sathasivam2003optimization}{26}
\bibcite{tho2010perceptron}{27}
\bibcite{van2011introduction}{28}
\bibcite{vanderplas2016python}{29}
\bibcite{veerarajan2007numerical}{30}
\bibcite{zaki2010advances}{31}
\bibcite{zheng2018feature}{32}
