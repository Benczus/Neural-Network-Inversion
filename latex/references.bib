@inproceedings{Fernandes2015API,
  title={A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News},
  author={Kelwin Fernandes and Pedro Vinagre and Paulo Cortez},
  booktitle={EPIA},
  year={2015}
}
@inproceedings{inproceedings,
	author = {Uddin, Md Taufeeq and Patwary, Muhammed and Ahsan, Tanveer and Alam, Mohammed Shamsul},
	year = {2016},
	month = {10},
	pages = {1-5},
	title = {Predicting the popularity of online news from content metadata},
	doi = {10.1109/ICISET.2016.7856498}
}
@article{Pedregosa2011ScikitlearnML,
	title={Scikit-learn: Machine Learning in Python},
	author={Fabian Pedregosa and Ga{\"e}l Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jacob VanderPlas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and Edouard Duchesnay},
	journal={Journal of Machine Learning Research},
	year={2011},
	volume={12},
	pages={2825-2830}
}
@article{article,
	author = {A. Jensen, Craig and D. Reed, Russell and J Marks, Robert and El-Sharkawi, Mohamed and Jung, Jae-byung and T. Miyamoto, Robert and M. Anderson, Gregory and Eggen, J},
	year = {2001},
	month = {02},
	pages = {},
	title = {Inversion of Feedforward Neural Networks: Algorithms And Applications}
}
@article{KINDERMANN1990277,
	title = "Inversion of neural networks by gradient descent",
	journal = "Parallel Computing",
	volume = "14",
	number = "3",
	pages = "277 - 286",
	year = "1990",
	issn = "0167-8191",
	doi = "https://doi.org/10.1016/0167-8191(90)90081-J",
	url = "http://www.sciencedirect.com/science/article/pii/016781919090081J",
	author = "J Kindermann and A Linden",
	keywords = "Inversion, multilayer perceptrons, backpropagation, generalization, robustness",
	abstract = "Inversion answers the question of which input patterns to a trained multilayer neural network approximate a given output target. This method is a tool for visualization of the information processing capability of a network stored in its weights. This knowledge about the network enables one to make informed decisions on the improvement of the training task and the choice of training sets. An inversion algorithm for multilayer perceptrons is derived from the backpropagation scheme. We apply inversion to networks for digit recognition. We observe that the multilayer perceptrons perform well with respect to generalization, i.e. correct classification of untrained digits. They are however bad on rejection of counterexamples, i.e. random patterns. Inversion gives an explanation for this drawback. We suggest an improved training scheme, and we show that a tradeoff exists between generalization and rejection of counterexamples."
}
@book{fine2006feedforward,
	title={Feedforward Neural Network Methodology},
	author={Fine, T.L.},
	isbn={9780387226491},
	lccn={98053188},
	series={Information Science and Statistics},
	url={https://books.google.hu/books?id=s-PlBwAAQBAJ},
	year={2006},
	publisher={Springer New York}
}
@book{tho2010perceptron,
	title={Perceptron Problem in Neural Network},
	author={Tho, D.X.},
	isbn={9783640648955},
	url={https://books.google.hu/books?id=ITK8MIItC2gC},
	year={2010},
	publisher={GRIN Verlag}
}
@book{mehrotra1997elements,
	title={Elements of Artificial Neural Networks},
	author={Mehrotra, K. and Ranka, K.M.C.K.M.S. and Mohan, C.K. and Ranka, S.},
	isbn={9780262133289},
	lccn={96020178},
	series={A Bradford book},
	url={https://books.google.hu/books?id=6d68Y4Wq\_R4C},
	year={1997},
	publisher={MIT Press}
}
@Inbook{Bottou2012,
	author="Bottou, L{\'e}on",
	editor="Montavon, Gr{\'e}goire
	and Orr, Genevi{\`e}ve B.
	and M{\"u}ller, Klaus-Robert",
	title="Stochastic Gradient Descent Tricks",
	bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
	year="2012",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="421--436",
	abstract="Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.",
	isbn="978-3-642-35289-8",
	doi="10.1007/978-3-642-35289-8_25",
	url="https://doi.org/10.1007/978-3-642-35289-8_25"
}

